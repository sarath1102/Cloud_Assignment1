Task 1: Cloud Infrastructure Setup in GCP: Task 1.1: Install Hadoop and create a Hadoop cluster #Create new user ‘hduser’ in namenode-1 instance #Login as root usersudo-iadduser hduser passwd hduser #Enable password and key based authentication vi /etc/ssh/sshd_configservice sshd restart# Add other 2 nodes ip and name in /etc/hosts file to communicate with names vi /etc/hosts# To find ip go to datanode-1 node’s sshhostname -i 10.142.0.3# To find ip go to datanode-2 node’s sshhostname -i 10.142.0.4#Create new user hduser in datanode-1sudo-iadduser hduserpasswd hduser # Enable password and key based authentication vi /etc/ssh/sshd_config service sshd restart # Add other 2 nodes ip and name in /etc/hosts file to communicate with names vi /etc/hosts # To find ip go to namenode-1 node’s ssh hostname -i 10.142.0.2 # Create new user hduser in datanode-2 sudo-iadduser hduserpasswd hduser# Enable password and key-based authenticationvi /etc/ssh/sshd_configservice sshd restart# Add other 2 nodes ip and name in /etc/hosts file to communicate with names vi /etc/hosts//In namenode-1sudo hdusercd ~ssh-keygen -t rsa -P “”ssh-copy-id -i/home/hduser/.ssh/id_rsa. pub hduser@datanode-1ssh-copy-id -i/home/hduser/.ssh/id_rsa. pub hduser@datanode-2ssh-copy-id -i/home/hduser/.ssh/id_rsa. pub hduser@namenode-1chmod 0600 ~/.ssh/authorized_keysssh datanode-1exitssh datanode-2 exit #Download Java and Hadoop 1. Get into the namenode -1 instance in GCP 2. Switch to root user sudo -i #Download java1.8 and Hadoop 2.7 by using the below command: cd /opt/wget --no-cookies --no-check-certificate --header "Cookie: gpw_e24=http%3A%2F%2Fwww.oracle.com%2F; oraclelicense=accept-securebackup- cookie"http://download.oracle.com/otnpub/java/jdk/8u131b11/d54c1d3a095b4ff2b6607d096f a80163/jdk-8u131-linux-x64.tar.gz wget https://archive.apache.org/dist/hadoop/core/hadoop-2.7.0/hadoop-2.7.0.tar.gz #To Install Java and extract java tar file tar xzf jdk-8u131-linux-x64.tar.gz cd /opt/jdk1.8.0_131 alternatives --install /usr/bin/java java /opt/jdk1.8.0_131/bin/java 2 alternatives --config java java -version #To Install Hadoop and Extract Hadoop tar file and update Hadoop configuration files using the below command cd /opt/tar xzf hadoop-2.7.0.tar.gzmv hadoop-2.7.0 /usr/local/Hadoopchown -R hduser:hduser /usr/local/Hadoopmkdir -p /usr/local/hadoop_store/tmpmkdir -p /usr/local/hadoop_store/hdfs/namenodemkdir -p /usr/local/hadoop_store/hdfs/datanodemkdir -p /usr/local/hadoop_store/hdfs/secondarynamenode chown -R hduser:hduser /usr/local/hadoop_store #Edit hdfs-site.xml file and add the given configuration vi/usr/local/hadoop/etc/hadoop/hdfs-site.xml <configuration><property><name>dfs.replication</name><value>3</value><description>Default block replication.The actual number of replications can be specified when the file is created. The default is used if replication is not specified in create time. </description> </property><property><name>dfs.namenode.name.dir</name> <value>file:/usr/local/hadoop_store/hdfs/namenode</value> </property><property><name>dfs.datanode.data.dir</name> <value>file:/usr/local/hadoop_store/hdfs/datanode</value> </property><property><name>dfs.namenode.checkpoint.dir</name> <value>file:/usr/local/hadoop_store/hdfs/secondarynamenode</value> </property><property><name>dfs.namenode.checkpoint.period</name> <value>3600</value></property></configuration> #Edit core-site.xml file and add the given configuration vi /usr/local/hadoop/etc/hadoop/core-site.xml <configuration><property><name>hadoop.tmp.dir</name> <value>/usr/local/hadoop_store/tmp</value><description>A base for other temporary directories.</description> </property> <property><name>fs.default.name</name><value>hdfs://namenode-1:54310</value><description>The name of the default file system. A URI whose scheme and authority determine the FileSystem implementation. The uri's scheme determines the config property fs.SCHEME.impl) naming the FileSystem implementation class. The uri's authority is used to determine the host, port, etc. for afilesystem.</description> </property> </configuration> #Edit map-site.xml file and add the given configuration vi /usr/local/hadoop/etc/hadoop/map-site.xml <configuration><property><name>mapreduce.framework.name</name><value>yarn</value></property><property><name>mapred.job.tracker</name> <value>namenode-1:54311</value><description>The host and port that the MapReduce job tracker runs at. If "local", then jobs are run in-process as a single mapand reduce task.</description></property></configuration> #Edit yarn-site.xml file and add the given configuration vi /usr/local/hadoop/etc/hadoop/yarn-site.xml <configuration><!-- Site specific YARN configuration properties --><property><name>yarn.resourcemanager.hostname</name> <value>namenode-1</value></property><property><name>yarn.nodemanager.aux-services</name> <value>mapreduce_shuffle</value></property><property> <name>yarn.nodemanager.aux-services.mapreduce.shuffle.class</name> <value>org.apache.hadoop.mapred.ShuffleHandler</value></property></configuration> #Add java path in hadoop-env.sh file echo 'export JAVA_HOME=/opt/jdk1.8.0_131' >> /usr/local/hadoop/etc/hadoop/hadoop- env.sh #Configure environment variable To run Hadoop services, we need to add hadoop and java path in the .basrhc file su hduservi /home/hduser/.bashrcexport HADOOP_PREFIX=/usr/local/hadoopexport HADOOP_HOME=/usr/local/hadoopexport HADOOP_MAPRED_HOME=${HADOOP_HOME}export HADOOP_COMMON_HOME=${HADOOP_HOME}export HADOOP_HDFS_HOME=${HADOOP_HOME}export YARN_HOME=${HADOOP_HOME}export HADOOP_CONF_DIR=${HADOOP_HOME}/etc/hadoop# Native Pathexport HADOOP_COMMON_LIB_NATIVE_DIR=${HADOOP_PREFIX}/lib/native export HADOOP_OPTS="-Djava.library.path=$HADOOP_PREFIX/lib"export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbinexport JAVA_HOME=/opt/jdk1.8.0_131export JRE_HOME=/opt/jdk1.8.0_131/jreexport PATH=$PATH:/opt/jdk1.8.0_131/bin:/opt/jdk1.8.0_131/jre/bin #Copy the Java tar file and Hadoop folder into another 2 instances (Datanode-1 and Datanode-2) su hdusercd /opt/scp jdk-8u131-linux-x64.tar.gz datanode-1:/home/hduser/ scp jdk-8u131-linux-x64.tar.gz datanode-2:/home/hduser/ #Copy Hadoop folder into another 2 instances cd /usr/localscp -r hadoop datanode-1:/home/hduser/scp -r hadoop_ store datanode-1:/home/hduser/scp -r /home/hduser/.bashrc datanode-1:/home/hduser/ scp -r /usr/local/hadoop datanode-2:/home/hduser/scp -r /usr/local/hadoop_store datanode-2:/home/hduser/scp -r /home/hduser/.bashrc datanode-2:/home/hduser/ #Get into another 2 instances , move folders in respective path and install java 1. Create and give permissions to the folders for Hadoop disk storage and processing sudo -i mv /home/hduser/jdk-8u131-linux-x64.tar.gz /opt/ mv /home/hduser/hadoop /usr/local/mv /home/hduser/hadoop_store /usr/local/chown -R hduser:hduser /usr/local/Hadoop chown -R hduser:hduser /usr/local/hadoop_store #Extract java tar file and configure java using the below command cd /opttar xzf jdk-8u131-linux-x64.tar.gzcd jdk1.8.0_131alternatives --install /usr/bin/java java /opt/jdk1.8.0_131/bin/java 2 alternatives --config java ##Repeat the step in both datanode-1 and datnode-2 Format namenode and start all Hadoop services 1.Add the datanodes instance name in the slaves file vi /usr/local/hadoop/etc/hadoop/slaves namenode-1datanode-1datanode-2 2. Get into the namenode instance and format namenode su hduserhadoop namenode -format3. Start storage and yarn services start-dfs.sh start-yarn.sh Task 1.2 Install MapReduce, Pig and Hive to use the cluster created in Task 1.1 #Pig installation: Switch to hdusersu - hduserwget http://www-eu.apache.org/dist/pig/pig-0.16.0/pig-0.16.0.tar.gz extract archivetar -xvf pig-0.16.0.tar.gzvi .bash_profileexport PIG_INSTALL=/home/hduser/pig-0.16.0export PATH=$PATH:/home/hduser/pig-0.16.0/binsource .bash_profilepig –version #hive Installation: su – hduser wget -b http://www-eu.apache.org/dist/hive/hive-1.2.2/apache-hive-1.2.2-bin.tar.gz tar -xvf apache-hive-1.2.2-bin.tar.gz vi .bash_profile export HIVE_HOME=/home/hduser/apache-hive-1.2.2-bin export PATH=$PATH:/home/hduser/apache-hive-1.2.2-bin/bin source .bash_profilehdfs dfs -mkdir -p /user/hive/warehouse hdfs dfs -chmod g+w /tmp vi $HIVE_HOME/bin/hive-config.shexport HADOOP_HOME=/home/hduser/hadoop-2.7.2 Task 2: Dataset Task 2.3: Load data into Gcloud technology #Loading Dataset from local path to Gcloudcat ~/.ssh/id_rsa.pubvim ~/.ssh/authorized_keys ifconfigcurl whatismyip.akamai.com pwd #In Local Path ssh-keygencat ~/.ssh/id_rsa.pubssh hduser@34.69.135.86 -vscp Youtube01-Psy.csv hduser@34.69.135.86:/home/hduser Task 3: Clean and process the data using Pig #Data cleaning using pig DEFINE CSVLoader org.apache.pig.piggybank.storage.CSVLoader(); data = LOAD '/user/Youtube01-Psy.csv ' USING org.apache.pig.piggybank.storage.CSVExcelStorage(',') as (comment_id:chararray, author:chararray, d:chararray, content:chararray, class:int); cleaned_data = FOREACH data GENERATE REPLACE($1, '[^a-zA-Z ]', '')as author, REPLACE($3, '[^a-zA-Z0-9 ]', ' ') as content, $4; STORE cleaned_data INTO ‘data_cleaned_full1’; Task 4: Ham and Spam using Pig Task 4.1: Query processed data to differentiate ham and spam part of the dataset Data1 = filter cleaned_data by (content matches '.*( subscribe | my channel| facebook|http).*'); store Data1 into '/user/hadoop/spam_dataset' using PigStorage('\t','-schema'); hadoop fs -getmerge /user/hadoop/spams_dataset ./spams.csv Data2 = filter cleaned_data by (content matches '.*( views | OPPA | good).*'); store Data2 into '/user/hadoop/ham_dataset' using PigStorage('\t','-schema'); hadoop fs -getmerge /user/hadoop/ham_dataset ./ham.csv Task 4.2: Find the top 10 spam accounts spam = GROUP Data1 by author; spam_count = FOREACH spam GENERATE group as author, COUNT(Data1) as total_comments; top_ten_spam1 = ORDER spam_count BY total_comments DESC; top_ten_spam = LIMIT top_ten_spam1 10 ;Task 4.3: Find the top 10 ham accountsham = GROUP Data2 by author; ham_count = FOREACH ham GENERATE group as author, COUNT(Data2) as total_comments; top_ten_ham1 = ORDER ham_count BY total_comments DESC; top_ten_ham = LIMIT top_ten_ham1 10 ; Task 5: TF-IDF using MapReduce tempdata = FOREACH data GENERATE $0,REPLACE($1, '[^a-zA-Z ]', '')as author, REPLACE($3, '[^a-zA-Z0-9 ]', ' ') as content, $4; temp1 = filter tempdata by (content matches '.*( subscribe | my channel| facebook|http).*'); store temp1 into '/user/hadoop/new_spam' using PigStorage('\t','-schema');hadoop fs -getmerge /user/hadoop/new_spam ./new_spam.csv tempdata = FOREACH data GENERATE $0,REPLACE($1, '[^a-zA-Z ]', '')as author, REPLACE($3, '[^a-zA-Z0-9 ]', ' ') as content, $4; temp2 = filter cleaned_data by (content matches '.*( views | OPPA | good).*'); store temp2 into '/user/hadoop/new_ham' using PigStorage('\t','-schema'); hadoop fs -getmerge /user/hadoop/new_ham ./new_ham.csv reducer-tfidf.py from operator import itemgetter import sys current_comment_id = None max_top_users = 10 max_top_words = 10 user_map = {} user_count_map = {} corpus_word_count = 0 old_word = None word_stats_dict = {} corpus_word_count_dict = {} num_documents = 0 # input comes from STDIN for line in sys.stdin: # remove leading and trailing whitespaceline = line.strip()# parse the input we got from our tf-idf mapper file comment_id, author, word_stat, count = line.split('\t') try: count = int(count) except ValueError: # count was not a number, so silently # ignore/discard this linecontinue ws = word_stat.split(",") word = ws[0] max_words = ws[1] # setup the user map dictionaries if not author in user_map: user_map[author] = {}if current_comment_id != comment_id: # we are receving a new stream for a new comment, increment# the author counter used to track how many comments a user made user_count_map[author] = user_count_map.get(author, 0) + 1 current_comment_id = comment_idnum_documents += 1 # also update user -> different word count tracker dict user_map[author][word] = user_map.get(author).get(word, 0) + count # store the word stats for each incoming word if old_word != word: if old_word:corpus_word_count_dict[old_word] = corpus_word_count corpus_word_count = 0 old_word = word try: corpus_word_count += int(count) if word in word_stats_dict.keys(): word_stats_dict[word].append(comment_id + "," + str(count) + "," + max_words) else: word_stats_dict[word] = list() word_stats_dict[word].append(comment_id + "," + str(count) + "," + max_words) except: continuecorpus_word_count_dict[old_word] = corpus_word_count # sort the users to start iterate from the user who had more commentssorted_users = {k: v for k, v in sorted(user_count_map.items(), reverse=True, key=lambda item: item[1])}user_counter = 0word_counter = 0for user, count in sorted_users.items(): if user_counter >= max_top_users: break user_counter += 1user_map_count = user_map.get(user, {})# for each user, sort our user -> word count tracker to get the top words sorted_word_count = {k: v for k, v in sorted(user_map_count.items(), reverse=True, key=lambda item: item[1])}for u, c in sorted_word_count.items(): if word_counter >= max_top_words: break word_counter += 1word_stats_list = word_stats_dict[u] # for each word, compute the tf-idf for word_stats in word_stats_list: word_stats = word_stats.split(",") try : term_frequency = int(word_stats[1])/int(word_stats[2]) inverse_doc_freq = num_documents / corpus_word_count_dict[u] tf_idf = term_frequency * inverse_doc_freqprint_key = u + "," + word_stats[0]print("%s\t%s" % (print_key, tf_idf)) except ZeroDivisionError as ex: print(word_stats[2]) print("Error", ex) else: continue word_counter = 0 mapper-tfidf.py import sys # input comes from STDIN (standard input) for line in sys.stdin: # remove leading and trailing whitespaceline = line.strip()# split the line to extract words and assign accordingly words = line.split("\t")comment_id = words[0]author = words[1]content = words[2]for word in content.split(" "): # split each word in the content and print to stdout for reducer to use it # we prints comment_id, author, word and total words in the comment if word.isalnum(): print("{}\t{}\t{},{}\t{}".format(comment_id, author, word.rstrip(), len(content.split(" ")), 1)) Task 5.1: MapReduce to calculate the TF-IDF of the top 10 spam keywords for each top 10 spam accounts hadoop jar /usr/local/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.7.0.jar -input /user/hadoop/new_spam1 -mapper "/home/hduser/mapper-tfidf.py" -reducer "/home/hduser/reducer-tfidf.py" -output ./tfidf-out hadoop fs -getmerge /user/hduser/tfidf-out ./ tfidf-out head -100 tfidf-out Task 5.2: MapReduce to calculate the TF-IDF of the top 10 ham keywords for each top 10 ham accounts hadoop jar /usr/local/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.7.0.jar -input /user/hadoop/new_ham1 -mapper "/home/hduser/mapper-tfidf.py" -reducer "/home/hduser/reducer-tfidf.py" -output ./out11 